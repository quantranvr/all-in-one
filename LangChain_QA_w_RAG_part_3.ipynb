{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "770X1887ZgaT",
        "HWHx5wLkbrEW"
      ],
      "authorship_tag": "ABX9TyNgvc9q0DjdvIzQYp4wbCsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantranvr/all-in-one/blob/main/LangChain_QA_w_RAG_part_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding logic for incorporating historical messages"
      ],
      "metadata": {
        "id": "OTRxghCVZWox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do so, we need to **update** our **prompt** to support historical messages as an input and add a sub-chain that takes the latest user question and **reformulates** it in a **context** from past messages"
      ],
      "metadata": {
        "id": "W63FUwgOimsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial @ https://python.langchain.com/docs/use_cases/question_answering/chat_history"
      ],
      "metadata": {
        "id": "pfn60sgeZekV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains 2 core parts:\n",
        "1. **Reproduce** [tutorial](https://python.langchain.com/docs/use_cases/question_answering/chat_history)'s example\n",
        "2. **Apply** knowledge learned to similar problem"
      ],
      "metadata": {
        "id": "YcLsEQEkZatN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "770X1887ZgaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQpll4eKZWDu",
        "outputId": "c2dfd297-4f99-4a99-a317-4711e5e005e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai langchainhub langchain-community chromadb bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Reproduce"
      ],
      "metadata": {
        "id": "HWHx5wLkbrEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "LkVvGXVqad5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "# split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# index\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# retrieve & generate\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI\n",
        "# chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# contextualize question\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, HumanMessage"
      ],
      "metadata": {
        "id": "Hr90DhpHbzHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "web_paths = (\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        ")\n",
        "\n",
        "bs4_strainer = bs4.SoupStrainer(\n",
        "    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        ")\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = web_paths,\n",
        "    bs_kwargs = {\"parse_only\": bs4_strainer}\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded document has {len(docs[0].page_content)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq0aIfv_cDqW",
        "outputId": "a64d6608-af33-44ee-dfe7-870503fd5d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded document has 42824 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split documents into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Number of chunks = {len(splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM6L7AimceD8",
        "outputId": "8e45a6d2-ae22-44e3-f90e-0ddecf029d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks = 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store and index chunks\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = splits,\n",
        "    embedding = OpenAIEmbeddings(),\n",
        ")"
      ],
      "metadata": {
        "id": "1z-b81RwdAhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve & generate\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "SArrQJ8FdjZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer question\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_O33gXYoemT8",
        "outputId": "4c2fda76-c2bd-4963-f952-33ef1c168cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through various methods such as using prompting techniques, task-specific instructions, or human inputs. The goal is to make the task more manageable and facilitate the interpretation of the model's thinking process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contextualize the question\n",
        "contextualize_q_system_prompt = \"\"\"\\\n",
        "Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\\\n",
        "\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_q_chain = (\n",
        "    contextualize_q_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "emxxKrx_eopi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [\n",
        "    HumanMessage(content=\"What does LLM stand for?\"),\n",
        "    AIMessage(content=\"Large language model\"),\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "p0AcCDnytl1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat prompt\n",
        "contextualize_q_prompt.invoke(\n",
        "    {\n",
        "        \"chat_history\": chat_history,\n",
        "        \"question\": \"What is meant by large\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTXl2nkwtxIj",
        "outputId": "491fe895-a191-4d62-ee84-290d5cfccc73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), HumanMessage(content='What does LLM stand for?'), AIMessage(content='Large language model'), HumanMessage(content='What is meant by large')])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# answer\n",
        "contextualize_q_chain.invoke(\n",
        "    {\n",
        "        \"chat_history\": chat_history,\n",
        "        \"question\": \"What is meant by large\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zX6ti2weryKt",
        "outputId": "94b3a581-5f0d-4b01-a5ab-ee6e5decffaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the definition of \"large\" in the context of a language model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain with chat history\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def contextualized_question(input: dict):\n",
        "    if input.get(\"chat_history\"):\n",
        "        return contextualize_q_chain\n",
        "    else:\n",
        "        return input[\"question\"]\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=contextualized_question | retriever | format_docs\n",
        "    )\n",
        "    | qa_prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "gDQ2YpdAsFbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer and add to history\n",
        "chat_history = []\n",
        "\n",
        "question = \"What is Task Decomposition?\"\n",
        "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
        "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
        "\n",
        "second_question = \"What are common ways of doing it?\"\n",
        "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86H93JYauKZP",
        "outputId": "8f269cef-100d-4eb5-ed93-85690d53c897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Common ways of task decomposition include:\\n1. Using techniques like Chain of Thought (CoT) or Tree of Thoughts, where the task is broken down into multiple manageable steps, allowing the model to think step by step and explore different reasoning possibilities.\\n2. Providing task-specific instructions or prompts to guide the model in decomposing the task. For example, asking the model to outline a story for writing a novel or asking for subgoals to achieve a specific task.\\n3. Involving human inputs, where humans provide guidance or input to help decompose the task into smaller steps. This can be done through collaboration or by leveraging human expertise in the task domain.')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: **Apply**"
      ],
      "metadata": {
        "id": "EDK8JxAcUQhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem:\n",
        "\n",
        "A LangChain learner wants to dive deeper into certain concepts by asking a series of related questions.\n",
        "\n",
        "Build a chatbot that could answer each of his questions based on information on LangChain official docs and his chat history with the chatbot"
      ],
      "metadata": {
        "id": "ebcvhB1UUoeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "-ve_u7mRuQYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f238dae7-32ff-4e47-e199-9e819bdcbf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load docs\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "# split into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# store and index\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# retrieve and generate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "Ck7wvoNZUb_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "\n",
        "web_paths = (\n",
        "    \"https://python.langchain.com/docs/modules/agents/\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/quick_start\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/concepts\",\n",
        "\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/openai_tools\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/xml_agent\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/json_agent\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/structured_chat\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/react\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search\",\n",
        "\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/custom_agent\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/streaming\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/agent_iter\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/agent_structured\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/handle_parsing_errors\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/intermediate_steps\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/max_iterations\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/max_time_limit\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/how_to/streaming_events\",\n",
        "\n",
        "    \"https://python.langchain.com/docs/modules/agents/tools/\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/tools/toolkits\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/tools/custom_tools\",\n",
        "    \"https://python.langchain.com/docs/modules/agents/tools/tools_as_openai_functions\",\n",
        ")\n",
        "\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"theme-doc-markdown markdown\"))\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = web_paths,\n",
        "    bs_kwargs = {\"parse_only\": bs4_strainer}\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Number of docs = {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYJgghnvUl3m",
        "outputId": "d22a6aaa-dd25-40db-9427-d1eee225ef2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of docs = 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap = 100,\n",
        "    add_start_index = True\n",
        ")\n",
        "\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Number of chunks = {len(splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrVSwl9MV9sa",
        "outputId": "5eb8f97c-a4db-4b3c-e456-b36891920bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks = 360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store and index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = splits,\n",
        "    embedding = OpenAIEmbeddings(),\n",
        ")"
      ],
      "metadata": {
        "id": "jGG5a89gWWU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve and generate\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "DiWQO3jyWqSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contextualize the question\n",
        "contextualize_q_system_prompt = \"\"\"\\\n",
        "Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, \\\n",
        "formulate a standalone question which can be understood \\\n",
        "without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_q_chain = (\n",
        "    contextualize_q_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "B_joIZgYXso4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test contextualize question chain\n",
        "contextualize_q_chain.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"What does LLM stand for?\"),\n",
        "            AIMessage(content=\"Large language model\"),\n",
        "        ],\n",
        "        \"question\": \"What is meant by large\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GPwVXxr1YjnW",
        "outputId": "896c7ca0-b5cc-4899-e75d-ddefae1ad5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the definition of \"large\" in the context of a language model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain with chat history\n",
        "qa_system_prompt = \"\"\"\\\n",
        "You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\\\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def contextualized_question(input: dict):\n",
        "    if input.get(\"chat_history\"):\n",
        "        return contextualize_q_chain\n",
        "    else:\n",
        "        return input[\"question\"]\n",
        "\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context = (\n",
        "            contextualized_question\n",
        "            | retriever\n",
        "            | format_docs\n",
        "        )\n",
        "    )\n",
        "    | qa_prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "FjmZE_4WYxkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "question_series = [\n",
        "    \"What is LangChain?\",\n",
        "    \"Could I use it with Python?\",\n",
        "    \"What are other programming languages that I could use it with?\",\n",
        "    \"How do I benefit from it?\",\n",
        "    \"What are key concepts of it?\",\n",
        "    \"What is the definition of each of them?\"\n",
        "]\n",
        "\n",
        "for question in question_series:\n",
        "    ai_msg = rag_chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"chat_history\": chat_history,\n",
        "    })\n",
        "\n",
        "    chat_history.extend([HumanMessage(content=question), ai_msg])"
      ],
      "metadata": {
        "id": "8hHN92SMZp9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xli41HxbJFE",
        "outputId": "ca223335-751e-4c12-ecc4-54e09661132d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is LangChain?'),\n",
              " AIMessage(content='LangChain is an open-source orchestration framework for building applications using large language models (LLMs) such as chatbots and virtual agents. It simplifies the programming and integration process with external data sources and software workflows. It supports Python and JavaScript languages and offers integrations for various LLM providers.'),\n",
              " HumanMessage(content='Could I use it with Python?'),\n",
              " AIMessage(content='Yes, LangChain supports Python as one of its supported languages. You can use LangChain to build applications and integrate with Python-based workflows and data sources.'),\n",
              " HumanMessage(content='What are other programming languages that I could use it with?'),\n",
              " AIMessage(content='LangChain supports Python and JavaScript as the programming languages for building applications.'),\n",
              " HumanMessage(content='How do I benefit from it?'),\n",
              " AIMessage(content='By using LangChain, you can leverage the power of large language models (LLMs) to create applications that can generate responses to user queries, such as answering questions or creating images from text prompts. This saves you time and effort compared to building custom LLMs from scratch. LangChain also simplifies the programming and integration process with external data sources and software workflows, making application development more efficient.'),\n",
              " HumanMessage(content='What are key concepts of it?'),\n",
              " AIMessage(content='Some key concepts of LangChain include its support for large language models (LLMs), its integration with external data sources and software workflows, and its provision of over 25 different embedding methods and over 50 different vector stores. LangChain also provides abstractions for common steps and concepts, simplifying the development process for applications using LLMs.'),\n",
              " HumanMessage(content='What is the definition of each of them?'),\n",
              " AIMessage(content='- Large Language Models (LLMs): These are powerful models that can understand and generate human-like text. LangChain supports the use of LLMs like chatbots and virtual agents in application development.\\n- External Data Sources and Software Workflows: LangChain allows integration with external data sources, such as databases or APIs, to fetch or store data. It also supports integration with software workflows, enabling seamless interaction with other software systems.\\n- Embedding Methods: Embedding methods are techniques used to represent words or sentences as numerical vectors. LangChain provides support for over 25 different embedding methods, allowing flexibility in how text is represented.\\n- Vector Stores: Vector stores are repositories or databases that store pre-computed vector representations of words or sentences. LangChain offers integrations with over 50 different vector stores, providing a wide range of options for storing and retrieving vector representations.')]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nr9HBUnCbJrb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}