{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxAaLo9w52DpsHda7gd88C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quantranvr/all-in-one/blob/main/LangChain_QA_w_RAG_part_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build chatbot that responses based on given websites and chat history"
      ],
      "metadata": {
        "id": "JKGX6jXZhxiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:**\n",
        "\n",
        "A singer found [[this article]](https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-a-complete-guide-2022)  about Spotify recommender interesting. However, he doesn't know much about the AI and Data world so he struggles to comprehend the article's content.\n",
        "\n",
        "Build a chatbot that could leverage the information appeared in the two articles to help him better understand what he could do to influence recommendation algorithms. **Remember to cite evidences from the articles that support chatbot's responses**.\n",
        "\n",
        "Keep in mind that he would want to ask a lot of questions, the later would likely to related to the previous one. Hence, t**he chatbot should be able to take into account the chat history**!"
      ],
      "metadata": {
        "id": "ttjuib-mjFK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Build chatbot without citation and chat history consideration (**done**)\n",
        "2. Build chatbot wit citation but no chat history consideration (**done**)\n",
        "3. Build chatbot with both citation and chat history consideration (**not yet**)"
      ],
      "metadata": {
        "id": "N2jO3gnqSkSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "rtqA_yq49YEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap output\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "m_emRLqSsUr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oXipTPjhFHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "17a0b1cb-8bf1-4cb5-ea30-cc146fa221cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/803.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hsk-C6e8G0Y94IQuCokhMPAWT3BlbkFJNIfI0QVOxOV6OsDveuto\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install\n",
        "!pip install --upgrade --quiet langchain langchain_community langchainhub langchain_openai chromadb bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# openai api key\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0FdNtw_ciXNX",
        "outputId": "95054a7d-1ec0-4a16-e5a3-54f8cf69425a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "kJEBux8K9bXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "# split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# store & index\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# retrieve and generate\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8c0PG1ieidPH",
        "outputId": "a357dcb9-7ab9-4964-d7b8-20505895d931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load documents"
      ],
      "metadata": {
        "id": "Som0c2Z49f5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "\n",
        "web_paths = (\n",
        "    \"https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-a-complete-guide-2022\",\n",
        ")\n",
        "\n",
        "bs_strainer = bs4.SoupStrainer(class_=(\"wrapper page-title\", \"article-rich-text w-richtext\"))\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = web_paths,\n",
        "    bs_kwargs = {\"parse_only\": bs_strainer}\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Number of docs = {len(docs)}\")\n",
        "\n",
        "total_chars = 0\n",
        "for i in range(len(docs)):\n",
        "    doc_chars = len(docs[i].page_content)\n",
        "    print(f\"Doc #{i+1} has {doc_chars} characters\")\n",
        "    total_chars += doc_chars\n",
        "\n",
        "print(f\"Total characters = {total_chars}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "MAHn1VVziwqR",
        "outputId": "0957ccff-c298-4d2e-c845-0d2e9f64225c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of docs = 1\n",
            "Doc #1 has 23706 characters\n",
            "Total characters = 23706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split into chunks"
      ],
      "metadata": {
        "id": "-IJ02yw39ie1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Number of chunks = {len(splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EReRdkfrldob",
        "outputId": "dd4a7b0a-7335-4b90-c46b-25efc727dd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks = 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store and index chunks"
      ],
      "metadata": {
        "id": "AMaG4Ha49kED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store and index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = splits,\n",
        "    embedding = OpenAIEmbeddings(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "I1j3n1C7mAWT",
        "outputId": "636e5897-1ddc-4531-8cdc-bd8af3f3a447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve and generate\n",
        "retriever = vectorstore.as_retriever()\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "trained_sentences = 2\n",
        "retrieved_sentences = 3\n",
        "\n",
        "instruction = \"\"\"\\\n",
        "You are an assistant for question-answering tasks. \\\n",
        "If user's input is not a question, behave like a kind and ready-to-help chatbot. \\\n",
        "Else, firstly use your trained knowledge to answer the question in at most two sentences, starting with \"From my knowledge, \". \\\n",
        "If you don't know then don't answer! \\\n",
        "Next, use the following pieces of retrieved context to answer the question in at most three sentences, starting with \"From the articles, \". \\\n",
        "If you don't know then say \"there is no information about that!\" \\\n",
        "Keep the answer concise. \\\n",
        "\"\"\"\n",
        "def format_rag_prompt(ragp, instruction):\n",
        "    contents = rag_prompt.messages[0].prompt.template.split(\"\\n\")\n",
        "    contents[0] = instruction\n",
        "    rag_prompt.messages[0].prompt.template = \"\\n\".join(contents)\n",
        "    return ragp\n",
        "\n",
        "rag_prompt = format_rag_prompt(rag_prompt, instruction)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CppiZ6xTm5Vy",
        "outputId": "19b1d8ca-e125-4fb9-eff9-6061484d0bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot 0 - without citation and chat history consideration"
      ],
      "metadata": {
        "id": "KJIX7gut9meO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chatbot_0: without citation and chat history consideration\n",
        "chatbot_0 = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8kQ0JT_rkdMU",
        "outputId": "eb18d37c-485a-419e-e0ad-96553c738e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test chatbot_0\n",
        "chatting = False\n",
        "while chatting:\n",
        "    print(\"You: \", end=\"\\n\")\n",
        "    question = input()\n",
        "    if \"thank you and see you again\" == question.lower():\n",
        "        print(\"\\nChatbot:\")\n",
        "        print(\"It is my pleasure to chat with you today!\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"\\nChatbot:\")\n",
        "        chatbot_0_msg = chatbot_0.invoke(question)\n",
        "        print(chatbot_0_msg)\n",
        "\n",
        "    print(\"\\n--------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Qu-jTJiGom02",
        "outputId": "5662eed5-2d07-4758-990a-ee41e11c7617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot 1 - With citation but no chat history consideration"
      ],
      "metadata": {
        "id": "c_oRx70Q9sGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_source(answer_from_docs, chunk):\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Is it true that \\\n",
        "        the following statement:\n",
        "        {statement}\n",
        "        is deduced from the following text:\n",
        "        {text}\n",
        "\n",
        "        Only answer with True or False!\n",
        "        \"\"\"\n",
        "    )\n",
        "    if \"true\" in llm.invoke(prompt.format(text=chunk.page_content, statement=answer_from_docs)).content.lower():\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B05He1qJqPM8",
        "outputId": "78ded140-9ac7-43b2-d975-6089ce2e28fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_proof(answer_from_docs, chunk):\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        the following statement:\n",
        "        {statement}\n",
        "        is deduced from the following text:\n",
        "        {text}\n",
        "\n",
        "        The proof of the statement can be found in which sentences in the text?\n",
        "        Use bullet list to list the sentences!\n",
        "        \"\"\"\n",
        "    )\n",
        "    return llm.invoke(prompt.format(text=chunk.page_content, statement=answer_from_docs)).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yqh0uqpms3oV",
        "outputId": "a29fb3cd-44a8-451f-f891-ff05614468ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def output_parser(output: dict):\n",
        "    full_ans = output[\"answer\"].content\n",
        "\n",
        "    rag_ans = [u for u in full_ans.split(\"\\n\\n\") if 'From the articles, ' in u]\n",
        "    if len(rag_ans) == 0:\n",
        "        return full_ans\n",
        "    else:\n",
        "        answer_from_docs = rag_ans[0].replace('From the articles, ', '')\n",
        "        chunks = output[\"context\"]\n",
        "        citations = []\n",
        "        for chunk in chunks:\n",
        "            if verify_source(answer_from_docs, chunk):\n",
        "                proof = get_proof(answer_from_docs, chunk)\n",
        "                proof = proof.replace(\"\\n\", \"\\n\\n\")\n",
        "                link = chunk.metadata['source']\n",
        "                citations.append(proof + \"\\n\\n\" + link)\n",
        "    ref = [\"References:\"] + citations\n",
        "    ref_str = \"\\n\\n\".join(ref)\n",
        "    return full_ans + \"\\n\\n\" + ref_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HE-fdWiPwC8j",
        "outputId": "be8c258d-bc5a-4db9-95d1-ebeea6bb148e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chatbot_1: with citation but no chat history consideration\n",
        "\n",
        "chatbot_1 = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ").assign(\n",
        "    answer = (\n",
        "        RunnablePassthrough.assign(context = lambda x: format_docs(x[\"context\"]))\n",
        "        | rag_prompt\n",
        "        | llm\n",
        "    )\n",
        ") | output_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cpmx5jquos7Z",
        "outputId": "aee4e4e4-3955-4db9-8aef-f55cc76f6744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test chatbot_1\n",
        "chatting = True\n",
        "while chatting:\n",
        "    print(\"You: \", end=\"\\n\")\n",
        "    question = input()\n",
        "    if \"thank you and see you again\" == question.lower():\n",
        "        print(\"\\nChatbot:\")\n",
        "        print(\"It is my pleasure to chat with you today!\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"\\nChatbot:\")\n",
        "        chatbot_1_msg = chatbot_1.invoke(question)\n",
        "        print(chatbot_1_msg)\n",
        "\n",
        "    print(\"\\n--------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qjj-X4PNluDE",
        "outputId": "0d852706-6beb-4acb-f49f-aa9502adfeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: \n",
            "Hello\n",
            "\n",
            "Chatbot:\n",
            "Hello! How can I assist you today?\n",
            "\n",
            "--------------------\n",
            "\n",
            "You: \n",
            "How Spotify's recommender use collaborative filtering?\n",
            "\n",
            "Chatbot:\n",
            "From my knowledge, Spotify's recommender uses collaborative filtering by comparing users' listening history and recommending songs that similar users have enjoyed.\n",
            "\n",
            "From the articles, Spotify's collaborative filtering model is trained on a sample of user-generated playlists, chosen based on the passion, care, love, and time users put into creating them. The algorithm compares users' listening history and recommends songs that similar users have enjoyed. However, Spotify has also moved towards focusing on the track's organizational similarity by studying playlist and listening session co-occurrence.\n",
            "\n",
            "References:\n",
            "\n",
            "- \"Today, the Spotify collaborative filtering model is trained on a sample of ~700 million user-generated playlists selected out of the much broader set of all user-generated playlists on the platform.\"\n",
            "\n",
            "- \"The main principle for choosing the playlists that make it into that sample? 'Passion, care, love, and time users put into creating those playlists.'\"\n",
            "\n",
            "- \"Now, we finally arrived at the point where the combination of collaborative and content-based approaches allows Spotify's recommender system to develop a holistic representation of the track.\"\n",
            "\n",
            "- \"At this point, the track profile is further enriched by combining the outputs of several independent algorithms to generate higher-level vectors (think of these as mood, genre, style tags, etc.).\"\n",
            "\n",
            "https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-a-complete-guide-2022\n",
            "\n",
            "--------------------\n",
            "\n",
            "You: \n",
            "What is the new approach of Spotify's recommender?\n",
            "\n",
            "Chatbot:\n",
            "From my knowledge, Spotify's new approach to recommender systems involves using track and user representations to find the perfect match between the two and serve relevant music to users.\n",
            "\n",
            "From the articles, Spotify's recommender system uses collaborative filtering, comparing users' listening history to recommend songs. They maintain a massive user-item interaction matrix to determine song similarity and user similarity. However, there is no information about any specific new approaches in the given context.\n",
            "\n",
            "References:\n",
            "\n",
            "- \"The DSP giant has pioneered the application of this so-called 'Netflix approach' in context of music recommendation — and widely publicized collaborative filtering as the driving power behind its recommendation engine.\"\n",
            "\n",
            "- \"The algorithm simply compares users' listening history: if user A has enjoyed songs X, Y and Z, and user B has enjoyed songs X and Y (but haven't heard Z yet), we should recommend song Z to them.\"\n",
            "\n",
            "- \"By maintaining a massive user-item interaction matrix covering all users and tracks on the platform, Spotify can tell if two songs are similar (if similar users listen to them) and if two users are similar (if they listen to the same songs).\"\n",
            "\n",
            "https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-a-complete-guide-2022\n",
            "\n",
            "--------------------\n",
            "\n",
            "You: \n",
            "What does the recommender do with raw audio signal?\n",
            "\n",
            "Chatbot:\n",
            "From my knowledge, the recommender uses raw audio signals to analyze the sonic characteristics of a track, such as danceability, energy, and valence, as well as the song's structure and melodic composition.\n",
            "\n",
            "From the articles, it is mentioned that the recommender system analyzes the raw audio signals of a track to generate the track's audio profile, including its primary audio features and an assessment of the song's structure and composition. Additionally, if there are lyrics, the recommender will analyze them using Natural Language Processing algorithms to establish the general theme and meaning of the lyrics.\n",
            "\n",
            "References:\n",
            "\n",
            "- \"the release is uploaded on the platform, Spotify will run raw audio analysis algorithms against the recording to generate the song's audio profile.\"\n",
            "\n",
            "- \"an educated assumption would be that state-of-art recommenders can generate highly detailed audio profiles for any given track — not only extracting the primary audio features (danceability, energy, valence, etc.) but also assessing the song's structure, melodic composition, and more.\"\n",
            "\n",
            "- \"The second aspect of the recording analyzed is the lyrics — if there are any, of course.\"\n",
            "\n",
            "- \"The recommender will analyze the text with Natural Language Processing algorithms to establish the general theme and meaning of the lyrics and extract any.\"\n",
            "\n",
            "https://www.music-tomorrow.com/blog/towards-recommender-system-optimization-how-can-artists-influence-recommendation-algorithms\n",
            "\n",
            "--------------------\n",
            "\n",
            "You: \n",
            "thank you and see you again\n",
            "\n",
            "Chatbot:\n",
            "It is my pleasure to chat with you today!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndejDuDxyBTQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}